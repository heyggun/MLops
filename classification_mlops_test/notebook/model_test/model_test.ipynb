{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e869963a-4e6f-4263-85e1-dd7745ecf0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20724/1625430661.py:26: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.29.2``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  loaded_model = mlflow.transformers.load_model(\n",
      "2023/07/05 05:12:37 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a61f74e4d7429eb7be6d9773246309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from minio import Minio\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "# raw_model, emoPuntu_removed_model, filtered_model\n",
    "model_list = [\n",
    "    \"s3://mlflow/mlflow/artifacts/9/209c25db460d48eba64907e293cbfb8b/artifacts/electra-test\",\n",
    "    's3://mlflow/mlflow/artifacts/9/c609e84536f14e70be4eee205991c04c/artifacts/electra-test',\n",
    "    \"s3://mlflow/mlflow/artifacts/9/302e3f9eb2714146a62665fe918fd31d/artifacts/electra-test\",\n",
    "]\n",
    "\n",
    "# raw_data, emoPuntu_removed_data, filtered_data\n",
    "data_list = [\n",
    "    \"test_phonenumber_data.csv\",\n",
    "    \"test_phonenumber_EmoPuntuRemoved.csv\",\n",
    "    \"test_phonenumber_PNfiltering.csv\",\n",
    "]\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = f\"http://minio-service.kubeflow.svc.cluster.local:9000\"\n",
    "\n",
    "mino_endpoint = os.environ.get(\n",
    "    \"MLFLOW_S3_ENDPOINT_URL\", \"http://61.80.148.154:30001\"\n",
    ").split(\"//\")[1]\n",
    "\n",
    "access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "client = Minio(\n",
    "    mino_endpoint,\n",
    "    access_key=access_key,\n",
    "    secret_key=secret_key,\n",
    "    secure=False,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def remove_emoji(s):\n",
    "    regrex_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    res = regrex_pattern.sub(r\"\", s)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_punctuation(s: str):\n",
    "    \"\"\"\n",
    "    A heuristic approach to check if len of input sentence below 30 after processing it.\n",
    "\n",
    "    Parameter:\n",
    "    --------\n",
    "    s : str\n",
    "        Input sentence to check if it's a spam or not\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    s : str\n",
    "        String that removed duplicated whitespaces and punctuations.\n",
    "    \"\"\"\n",
    "    regex_list = [\n",
    "        r\"([ㄱ-ㅎㅏ-ㅣ`~!@#$%^&*()_♡\\+\\=\\-,./<'\\]'>?;'\\\\:\\|'[''{''}'\\\"\\'])\",\n",
    "        r\"[^\\w\\s]\",\n",
    "    ]\n",
    "    for r in regex_list:\n",
    "        s = re.sub(r, \"\", s)\n",
    "\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "\n",
    "def phonenumber_filter(s: str):\n",
    "    \"\"\"\n",
    "    A heuristic approach to check if input string contains any phone number.\n",
    "\n",
    "    Parameter:\n",
    "    --------\n",
    "    s : str\n",
    "        Input sentence that might contain phone number\n",
    "    Return:\n",
    "    --------\n",
    "    res_three : re.Match\n",
    "    res_double : re.Match\n",
    "        Part of sentence that may contain possible phone number\n",
    "    \"\"\"\n",
    "    # Remove newline\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "\n",
    "    first_group = [\n",
    "        r\"([영공0])\",\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "        r\"(.{0,20})\",\n",
    "    ]\n",
    "\n",
    "    second_group = [\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "    ]\n",
    "\n",
    "    third_group = [\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "        r\"(.{0,20})\",\n",
    "        r\"([영공일이삼사오육륙칠팔구|0-9])\",\n",
    "    ]\n",
    "\n",
    "    # 1. Check if the processed sentence has phone number pattern\n",
    "    regex_one = re.compile(\n",
    "        \"\".join(first_group) + \"\".join(second_group) + \"\".join(third_group)\n",
    "    )\n",
    "    regex_two = re.compile(\"\".join(second_group) + \"\".join(third_group))\n",
    "\n",
    "    res_three = re.search(regex_one, s)\n",
    "    res_double = re.search(regex_two, s)\n",
    "\n",
    "    return (res_three, res_double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4164a413-2d88-44f7-9739-0c216cc30b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    누나들이좋아여 \\n 큰거좋아하는누나들만 \\n 공일공 육육일오 오팔공칠 연락줘여\n",
      "Name: 0, dtype: object\n",
      "\n",
      "precision: 0.9423076923076923 \n",
      "recall: 0.98 \n",
      "f1: 0.9607843137254902 \n",
      "acc: 0.96\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"test_phonenumber_data.csv\")\n",
    "\n",
    "for index, row  in data.iterrows():\n",
    "    if remove_punctuation(remove_emoji(row[\"conts\"])) == \"\":\n",
    "        data.loc[index, \"conts\"] = np.NaN\n",
    "    else:\n",
    "        data.loc[index, \"conts\"] = remove_punctuation(remove_emoji(row[\"conts\"]))\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Save \n",
    "data.to_csv(\"test_phonenumber_EmoPuntuRemoved.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# Read\n",
    "read_data = pd.read_csv(\"test_phonenumber_EmoPuntuRemoved.csv\")\n",
    "\n",
    "# Sanity check\n",
    "assert len(data) == len(read_data), \"Sanity Check failed\"\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    assert data.loc[index, \"conts\"] == read_data.loc[index, \"conts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d36cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"test_phonenumber_data.csv\")\n",
    "\n",
    "for index, row  in data.iterrows():\n",
    "    if remove_punctuation(remove_emoji(row[\"conts\"])) == \"\":\n",
    "        data.loc[index, \"conts\"] = np.NaN\n",
    "    else:\n",
    "        data.loc[index, \"conts\"] = remove_punctuation(remove_emoji(row[\"conts\"]))\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "def filtering(input):\n",
    "    three, double = phonenumber_filter(input)\n",
    "\n",
    "    if not (three or double):\n",
    "        return input\n",
    "    elif not three:\n",
    "        return double.group()\n",
    "    elif not double:\n",
    "        return three.group()\n",
    "    else:\n",
    "        three_txt, double_txt = three.group(), double.group()\n",
    "        if len(three_txt) > len(double_txt):\n",
    "            return three_txt\n",
    "        else:\n",
    "            return double_txt\n",
    "\n",
    "data[\"conts\"] = data[\"conts\"].apply(filtering)\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Save \n",
    "data.to_csv(\"test_phonenumber_PNfiltering.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# Read\n",
    "read_data = pd.read_csv(\"test_phonenumber_PNfiltering.csv\")\n",
    "\n",
    "# Sanity check\n",
    "assert len(data) == len(read_data), \"Sanity Check failed\"\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    assert data.loc[index, \"conts\"] == read_data.loc[index, \"conts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d53179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model as a PyFuncModel.\n",
    "# mlflow.pyfunc.get_model_dependencies(logged_model)\n",
    "\n",
    "for model, data in zip(model_list, data_list):\n",
    "    loaded_model = mlflow.transformers.load_model(model_uri=model)\n",
    "    loaded_data = pd.read_csv(data)\n",
    "    \n",
    "    y_pred = [1 if loaded_model.predict(i)[0][\"label\"]==\"LABEL_1\" else 0 for i in loaded_data[\"conts\"].values]\n",
    "    y_true = loaded_data[\"label\"].values\n",
    "\n",
    "    assert len(y_pred) == len(y_true), \"y_pred and y_true have to be the same length\"\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != y_true[i]:\n",
    "            print(f\"pred:{y_pred[i]}, true:{y_true[i]} \\t{loaded_data.loc[i,'conts']}\")\n",
    "        \n",
    "    \n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    confusion = confusion_matrix(y_true, y_pred, normalize='all')\n",
    "    \n",
    "    print(f\"\\nmodel:{model} \\ndata:{loaded_data.loc[1,'conts']}\\nprecision: {precision} \\nrecall: {recall} \\nf1: {f1} \\nacc: {acc} \\nconfusion_matrix(tn, fp, fn, tp): {confusion.ravel()}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
