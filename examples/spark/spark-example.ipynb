{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8441f32-ea54-4aec-b3f7-3f79d2111d74",
   "metadata": {},
   "source": [
    "# Programming Model\n",
    "The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627a582-400a-4e8e-afa0-51695cae0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7dcf90-b38f-45af-a824-59e586abf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8cc724-fb1f-40b1-a402-bd724339df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7fae6-24f8-43ef-90a0-01d2b50f4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-service.default.svc.cluster.local:9092\") \\\n",
    "    .option(\"subscribe\", \"pn_classification\") \\\n",
    "    .load()\n",
    "\n",
    "print(lines.isStreaming) # True for DataFrames that have streaming sources\n",
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b90280-9cee-4d8b-bb20-a54b7a285124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "    explode(split(lines.value, \" \")).alias(\"word\")\n",
    ")\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db923f-ebb2-4ba9-b9ba-91b4031c3617",
   "metadata": {},
   "source": [
    "# Output is defined as what gets written out to the external storage.\n",
    "    Complete Mode: The entire updated Result Table will be written to the external storage.\n",
    "    Append Mode: Only the new rows appended in the Result Tables since the last trigger will be written to the external storage.\n",
    "    Update Mode: Only the rows that were updated in the Result Table since the last trigger will be written to the external storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaac3b7-a14c-40e7-9ac8-ef2d2f45923f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "# Wait for the termination of the query using awaitTermination() to prevent the process from exiting while the query is active.\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58daae4-b16a-425d-a18d-1ca39c94f40f",
   "metadata": {},
   "source": [
    "# API using Datasets and DataFrames\n",
    "DataFrames and Datasets can represent static, bounded data, as well as streaming, unbounded data. Similar to static Datasets/DataFrames, you can use the common entry point \"SparkSession\" to create streaming DataFrames/Datasets from streaming sources, and apply the same operations on them as static DataFrames/Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5655e2-1fbb-4283-8b65-da4db9f014d7",
   "metadata": {},
   "source": [
    "# Creating streaming DataFrames and streaming Datasets\n",
    "Streaming DataFrames can be created through the DataStreamReader interface returned by SparkSession.readStream().\n",
    "## Input source\n",
    "    File source\n",
    "    Kafka source\n",
    "    Socket source (for testing)\n",
    "    Rate source (for testing)\n",
    "    Rate Per Micro-Batch source (for testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e40c86-5c69-40ea-aadf-819f9a72fb7f",
   "metadata": {},
   "source": [
    "# Operations on streaming DataFrames/Datasets\n",
    "You can apply all kinds of operations on streaming DataFrames/Datasets - ranging from untyped, SQL-like operations, to typed-RDD-like operations(map, filter, flatMap)\n",
    "See the https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620e2c5-3bac-4299-aa9e-f0607ae033e2",
   "metadata": {},
   "source": [
    "# For Kafka\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeedbd8-7a14-48ce-a75c-8970acb8f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import window\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3389975-163b-4511-bdd8-547f0c3fbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-test-Kafka\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # Subscribe to 1 topic\n",
    "# df = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"kafka-service.default.svc.cluster.local:9092\") \\\n",
    "#     .option(\"subscribe\", \"pn_classification\") \\\n",
    "#     .load()\n",
    "# df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# # Subscribe to 1 topic, with headers\n",
    "# df = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"kafka-service.default.svc.cluster.local:9092\") \\\n",
    "#     .option(\"subscribe\", \"pn_classification\") \\\n",
    "#     .option(\"includeHeaders\", \"true\") \\\n",
    "#     .load()\n",
    "# df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\")\n",
    "\n",
    "# # Subscribe to multiple topics\n",
    "# df = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"kafka-service.default.svc.cluster.local:9092\") \\\n",
    "#     .option(\"subscribe\", \"pn_classification,pn_classification_1\") \\\n",
    "#     .load()\n",
    "# df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to a pattern\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-service.default.svc.cluster.local:9092\") \\\n",
    "    .option(\"subscribePattern\", \"pn_*\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9f74b-be71-4cad-a8f4-8d6eff35bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958860f-8241-4141-a4a6-cb6cc54c34ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e0033-a420-4e6e-b758-bb63507509cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "userSchema = StructType() \\\n",
    "    .add(\"from\", \"string\") \\\n",
    "    .add(\"test-data-Title\", \"string\") \\\n",
    "    .add(\"test-data-device\", \"string\") \\\n",
    "    .add(\"index\", \"integer\") \\\n",
    "    .add(\"time\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5bcaf-5cc9-4049-8e77-b56bb58ee18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba0220-89da-4c58-97d0-8402677cfd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e680e-aa9f-464b-9181-c94a7e53c3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff66687-b318-4547-9f3e-9783a8c1bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "# Wait for the termination of the query using awaitTermination() to prevent the process from exiting while the query is active.\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bffb014-6ecc-4d70-b4bb-01e4177b6740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic operations - Selection, Projection, Aggregation\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Custom Schema\n",
    "userSchema = StructType() \\\n",
    "    .add(\"from\", \"string\") \\\n",
    "    .add(\"test-data-Title\", \"string\") \\\n",
    "    .add(\"test-data-device\", \"string\") \\\n",
    "    .add(\"index\", \"integer\") \\\n",
    "    .add(\"time\", \"string\")\n",
    "\n",
    "# Create DataFrame representing the stream of input\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-service.default.svc.cluster.local:9092\") \\\n",
    "    .option(\"subscribe\", \"pn_classification\") \\\n",
    "    .load()\n",
    "\n",
    "print(df.isStreaming) # True for DataFrames that have streaming sources\n",
    "df.printSchema()\n",
    "udf = spark.udf.register(\"decode\", lambda x : x.decode(\"utf-8\"))\n",
    "df = df.withColumn(\"value\", udf(df[\"value\"]))\n",
    "#df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "#df = df.select(col(\"value\").cast(\"string\").alias(\"tmp\")).select(from_json(col('tmp'), userSchema))\n",
    "\n",
    "# todo make unbounded_table\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce02336-2664-47e2-b0aa-a2d10368e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, base64\n",
    "\n",
    "df = df.withColumn(\"\", base64(col(\"value\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
